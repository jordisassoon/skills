{
  "Python": {
    "completion": 100,
    "mandatory_resources": [
       
    ],
    "optional_resources": [
    
    ],
    "depends_on": []
  },
  "PyTorch": {
    "completion": 100,
    "mandatory_resources": [

    ],
    "optional_resources": [
    
    ],
    "depends_on": ["Python"]
  },
  "CUDA": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "CUDA C++ Class Part 1",
        "url": "https://www.youtube.com/watch?v=Sdjn9FOkhnA"
      },
      {
        "name": "CUDA C++ Class Part 2",
        "url": "https://www.youtube.com/watch?v=pyW9St8uM8w"
      },
      {
        "name": "CUDA C++ Class Part 3",
        "url": "https://www.youtube.com/watch?v=kTWoGCSugB4"
      },
      {
        "name": "CUDA C++ Class Part 4",
        "url": "https://www.youtube.com/watch?v=H6jJ9mXW1eI"
      },
      {
        "name": "CUDA by Example Book",
        "url": "https://edoras.sdsu.edu/~mthomas/docs/cuda/cuda_by_example.book.pdf"
      }
    ],
    "optional_resources": [
      {
        "name": "CUDA Programming Course",      
        "url": "https://youtube.com/watch?v=86FAWCzIe_4&t=667s&pp=ygUEY3VkYQ%3D%3D"
      },
      {
        "name": "Interview with NVIDIA CUDA Architect Stephen Jones",
        "url": "https://www.youtube.com/watch?v=dNUMNifgExs"
      },
      {
        "name": "CUDA Training Series by NVIDIA",
        "url": "https://www.olcf.ornl.gov/cuda-training-series/"
      },
      {
        "name": "CUDA Application Design and Development Book",
        "url": "https://vowi.fsinf.at/images/9/97/TU_Wien-GPU_Architectures_and_Programming_VU_%28Bartocci%29_-_CUDA_Application_Design_and_Development.pdf"
      },
      {
        "name": "Professional CUDA C Programming Book",
        "url": "https://www.cs.utexas.edu/~rossbach/cs380p/papers/cuda-programming.pdf"
      }
    ],
    "depends_on": ["PyTorch", "C++", "C", "NCCL"]
  },
  "NCCL": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "Scaling Deep Learning Training with NCCL",
        "url": "https://developer.nvidia.com/blog/scaling-deep-learning-training-nccl"
      },
      {
        "name":"Basics of NCCL",
        "url": "https://www.youtube.com/watch?v=T22e3fgit-A"
      },
      {
        "name": "NCCL: The Inter-GPU Communication Library Powering Multi-GPU AI",
        "url": "https://www.nvidia.com/en-us/on-demand/session/gtc25-s72583/"
      },
      {
        "name": "Multi-GPU Programming in NCCL and NVSHMEM",
        "url": "https://www.youtube.com/live/2xMzQ1Z2Qe0"
      },
      {
        "name": "Tutorial: GPU Communication Libraries for Accelerating HPC and AI Applications",
        "url": "https://www.youtube.com/watch?v=rlA5QreHekk&list=PLBM5Lly_T4yRGBFgforeMTDpjasC_PV7r"
      }
    ],
    "optional_resources": [
      {
        "name": "NCCL Documentation",
        "url": "https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/index.html"
      }
    ],
    "depends_on": []
  },
  "Triton": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "Triton Documentation",
        "url": "https://triton-lang.org/"
      },
      {
        "name": "Triton Tutorial",
        "url": "https://github.com/VikParuchuri/triton_tutorial"
      },
      {
        "name": "Practitioner's Guide to Triton",
        "url": "https://www.youtube.com/watch?v=DdTsX6DQk24"
      }
    ],
    "optional_resources": [
      {
        "name": "Triton Resources List",
        "url": "https://github.com/rkinas/triton-resources"
      }
    ],
    "depends_on": ["CUDA"]
  },
  "C++": {
    "completion": 0,
    "mandatory_resources": [

    ],
    "optional_resources": [
      {
        "name": "learncpp.com",
        "url": "https://learncpp.com/"
      }
    ],
    "depends_on": []
  },
  "C": {
    "completion": 0,
    "mandatory_resources": [

    ],
    "optional_resources": [

    ],
    "depends_on": []
  },
  "Deep Language Learning": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "DeepSeek V3 Technical Paper",
        "url": "https://arxiv.org/abs/2412.19437v1"
      },
      {
        "name": "Olmo 3 Paper",
        "url": "https://arxiv.org/abs/2512.13961"
      },
      {
        "name": "PaLM 2 Paper",
        "url": "https://arxiv.org/abs/2305.10403"
      },
      {
        "name": "LLaMA 3 Paper",
        "url": "https://arxiv.org/abs/2407.21783"
      },
      {
        "name": "Attention Is All You Need Paper",
        "url": "https://arxiv.org/abs/1706.03762"
      },
      {
        "name": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding Paper",
        "url": "https://arxiv.org/abs/1810.04805"
      },
      {
        "name": "Language Models are Few-Shot Learners Paper",
        "url": "https://arxiv.org/abs/2005.14165"
      },
      {
        "name": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer Paper",
        "url": "https://arxiv.org/abs/1910.10683"
      },
      {
        "name": "Scaling Laws for Neural Language Models Paper",
        "url": "https://arxiv.org/abs/2001.08361"
      },
      {
        "name": "Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks Paper",
        "url": "https://arxiv.org/abs/2005.11401"
      },
      {
        "name": "LoRA: Low-Rank Adaptation of Large Language Models Paper",
        "url": "https://arxiv.org/abs/2106.09685"
      },
      {
        "name": "Chain-of-Thought Prompting Elicits Reasoning in Large Language Models Paper",
        "url": "https://arxiv.org/abs/2201.11903"
      },
      {
        "name": "Self-Consistent Improves Chain of Thought Reasoning in Language Models Paper",
        "url": "https://arxiv.org/abs/2203.11171"
      },
      {
        "name": "Language Models (Mostly) Know What They Know Paper",
        "url": "https://arxiv.org/abs/2207.05221"
      },
      {
        "name": "Training language models to follow instructions with human feedback Paper",
        "url": "https://arxiv.org/abs/2203.02155"
      },
      {
        "name": "Toolformer: Language Models Can Teach Themselves to Use Tools Paper",
        "url": "https://arxiv.org/abs/2302.04761"
      },
      {
        "name": "ColBERTv2: Effective and Efficient Retrieval via Lightweight Late Interaction",
        "url": "https://arxiv.org/abs/2112.01488"
      },
      {
        "name": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena Paper",
        "url": "https://arxiv.org/abs/2306.05685"
      }
    ],
    "optional_resources": [
      {
        "name": "Machine Learning Engineering Open Book",
        "url": "https://github.com/stas00/ml-engineering"
      }
    ],
    "depends_on": ["PyTorch"]
  },
  "Massively Parallel Processing": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "Programming Massively Parallel Processors: A Hands-on Approach",
        "url": "https://www.cse.iitd.ac.in/~rijurekha/col851/necessary_reading/cudabook.pdf"
      },
      {
        "name": "Making Deep Learning Go Brrr From First Principles",
        "url": "https://horace.io/brrr_intro.html"
      },
      {
        "name": "GPUs Go Brrr",
        "url": "https://hazyresearch.stanford.edu/blog/2024-05-12-tk"
      },
      {
        "name": "HuggingFace Ultra-Scale Playbook",
        "url": "https://huggingface.co/spaces/nanotron/ultrascale-playbook"
      },
      {
        "name": "DeepSeek V3 Technical Paper, Section 3",
        "url": "https://arxiv.org/abs/2412.19437v1"
      },
      {
        "name": "TorchTitan",
        "url": "https://openreview.net/forum?id=SFN6Wm7YBI"
      },
      {
        "name": "Bringing HPC Techniques to Deep Learning",
        "url": "https://andrew.gibiansky.com/blog/machine-learning/baidu-allreduce/"
      },
      {
        "name": "Ring Attention Explained",
        "url": "https://coconut-mode.com/posts/ring-attention/"
      }
    ],
    "optional_resources": [
      {
        "name": "Ring Flash Attention Repo",
        "url": "https://github.com/zhuzilin/ring-flash-attention"
      },
      {
        "name": "Visualizing 6D Mesh Parallelism",
        "url": "https://main-horse.github.io/posts/visualizing-6d/"
      },
      {
        "name": "How to Scale Your Model: A Systems View of LLMs on TPUs by Google",
        "url": "https://jax-ml.github.io/scaling-book/"
      }
    ],
    "depends_on": ["CUDA", "NCCL", "Hardware"]
  },
  "Context Parallelism": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "HuggingFace Ultra-Scale Playbook CP section",
        "url": "https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=context_parallelism"
      }
    ],
    "optional_resources": [

    ],
    "depends_on": ["Deep Language Learning", "Massively Parallel Processing"]
  },
  "Data Parallelism": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "HuggingFace Ultra-Scale Playbook DP section",
        "url": "https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=data_parallelism"
      },
      {
        "name": "Data-Parallel Distributed Training of Deep Learning Models",
        "url": "https://siboehm.com/articles/22/data-parallel-training"
      }
    ],
    "optional_resources": [

    ],
    "depends_on": ["Deep Language Learning", "Massively Parallel Processing"]
  },
  "Tensor Parallelism": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "HuggingFace Ultra-Scale Playbook TP section",
        "url": "https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism"
      },
      {
        "name": "Reducing Activation Recomputation in Large Transformer Models",
        "url": "https://arxiv.org/abs/2205.05198"
      }
    ],
    "optional_resources": [

    ],
    "depends_on": ["Deep Language Learning", "Massively Parallel Processing"]
  },
  "ZeRO (FSDP)": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "HuggingFace Ultra-Scale Playbook ZeRO section",
        "url": "https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=zero_fully_sharded_data_parallel"
      },
      {
        "name": "ZeRO: Memory Optimization Towards Training A Trillion Parameter Models",
        "url": "https://arxiv.org/abs/1910.02054"
      },
      {
        "name": "PyTorch FSDP: Experiences on Scaling Fully Sharded Data Parallel",
        "url": "https://arxiv.org/abs/2304.11277"
      }
    ],
    "optional_resources": [
      {
        "name": "DeepSpeed ZeRO vs 3D Parallelism",
        "url": "https://www.deepspeed.ai/tutorials/large-models-w-deepspeed/#understanding-performance-tradeoff-between-zero-and-3d-parallelism"
      },
      {
        "name": "FSDP in 500 LOC",
        "url": "https://github.com/facebookresearch/capi/blob/main/fsdp.py"
      }
    ],
    "depends_on": ["Deep Language Learning", "Massively Parallel Processing"]
  },
  "Pipeline Parallelism": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "HuggingFace Ultra-Scale Playbook PP section",
        "url": "https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=pipeline_parallelism"
      },
      {
        "name": "NVIDIA Technical Blog on Pipeline Parallelism",
        "url": "https://developer.nvidia.com/blog/scaling-language-model-training-to-a-trillion-parameters-using-megatron/#pipeline_parallelism"
      },
      {
        "name": "Breadth-First Pipeline Parallelism",
        "url": "https://arxiv.org/abs/2211.05953"
      }
    ],
    "optional_resources": [

    ],
    "depends_on": ["Deep Language Learning", "Massively Parallel Processing"]
  },
  "Expert Parallelism": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "HuggingFace Ultra-Scale Playbook EP section",
        "url": "https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=expert_parallelism"
      }
    ],
    "optional_resources": [

    ],
    "depends_on": ["Massively Parallel Processing", "MoE"]
  },
  "MoE": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "HuggingFace MoE Blog",
        "url": "https://huggingface.co/blog/moe"
      },
      {
        "name": "MegaBlocks: Efficient Sparse Training with Mixture-of-Experts",
        "url": "https://arxiv.org/abs/2211.15841"
      },
      {
        "name": "FasterMoE: modeling and optimizing training of large-scale dynamic pre-trained models",
        "url": "https://dl.acm.org/doi/10.1145/3503221.3508418"
      }
    ],
    "optional_resources": [

    ],
    "depends_on": ["Deep Language Learning"]
  },
  "Hardware": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "How do Graphics Cards Work? Exploring GPU Architecture",
        "url": "https://www.youtube.com/watch?v=h9Z4oGN89MU"
      },
      {
        "name": "Modal GPU Glossary",
        "url": "https://modal.com/gpu-glossary"
      },
      {
        "name": "100,000 H100 Clusters",
        "url": "https://newsletter.semianalysis.com/p/100000-h100-clusters-power-network"
      },
      {
        "name": "Fire-Flyer AI-HPC",
        "url": "https://www.arxiv.org/abs/2408.14158"
      },
      {
        "name": "Performance Hints",
        "url": "https://abseil.io/fast/hints.html"
      }
    ],
    "optional_resources": [
      {
        "name": "Building Metaâ€™s GenAI Infrastructure",
        "url": "https://engineering.fb.com/2024/03/12/data-center-engineering/building-metas-genai-infrastructure/"
      }
    ],
    "depends_on": []
  },
  "Data Types": {
    "completion": 0,
    "mandatory_resources": [

    ],
    "optional_resources": [

    ],
    "depends_on": ["Hardware"]
  },
  "Quantization": {
    "completion": 0,
    "mandatory_resources": [
      {
        "name": "Mixed Precision Training",
        "url": "https://arxiv.org/abs/1710.03740"
      }
    ],
    "optional_resources": [

    ],
    "depends_on": ["Data Types", "Deep Language Learning"]
  },
  "Profiling and Benchmarking": {
    "completion": 0,
    "mandatory_resources": [

    ],
    "optional_resources": [

    ],
    "depends_on": ["PyTorch", "CUDA", "NCCL", "Hardware"]
  },
  "Compilers": {
    "completion": 0,
    "mandatory_resources": [

    ],
    "optional_resources": [

    ],
    "depends_on": ["C", "C++", "Python", "Hardware", "Massively Parallel Processing"]
  },
  "Cluster Topology": {
    "completion": 0,
    "mandatory_resources": [

    ],
    "optional_resources": [

    ],
    "depends_on": ["Massively Parallel Processing"]
  },
  "Scheduling": {
    "completion": 0,
    "mandatory_resources": [

    ],
    "optional_resources": [

    ],
    "depends_on": ["Massively Parallel Processing"]
  },
  "Fault Tolerance": {
    "completion": 0,
    "mandatory_resources": [

    ],
    "optional_resources": [

    ],
    "depends_on": ["Massively Parallel Processing", "Deep Language Learning"]
  },
  "Cost Estimation": {
    "completion": 0,
    "mandatory_resources": [

    ],
    "optional_resources": [

    ],
    "depends_on": ["Deep Language Learning", "Massively Parallel Processing"]
  },
  "Workload Management": {
    "completion": 0,
    "mandatory_resources": [

    ],
    "optional_resources": [
      
    ],
    "depends_on": ["Deep Language Learning", "Massively Parallel Processing"]
  },
  "Kernels": {
    "completion": 0,
    "mandatory_resources": [

    ],
    "optional_resources": [
      
    ],
    "depends_on": ["CUDA", "Triton", "Massively Parallel Processing", "Deep Language Learning"]
  }
}
